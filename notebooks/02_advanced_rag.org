#+TITLE: Module 2: Advanced RAG Techniques
#+AUTHOR: Jason Walsh
#+EMAIL: j@wal.sh
#+PROPERTY: header-args:python :results output :mkdirp yes

* Workshop Requirements - Lab 2: Improve Accuracy and Safety

** Learning Objectives
- Implement metadata filtering for precise retrieval
- Add guardrails for content safety
- Use reranking to improve result quality
- Dynamic entity extraction for intelligent filtering

** Prerequisites Validation

#+BEGIN_SRC shell
# 1. Verify Lab 1 Knowledge Bases exist
aws bedrock-agent list-knowledge-bases \
  --query "knowledgeBaseSummaries[?contains(name, 'advanced-rag')].knowledgeBaseId" \
  --output text

# 2. Check if guardrails are available in your region
aws bedrock list-guardrails --max-results 1 2>/dev/null && echo "Guardrails API available" || echo "Guardrails not available"

# 3. Verify metadata in S3 documents
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
S3_BUCKET="${ACCOUNT_ID}-us-west-2-advanced-rag-workshop"
aws s3 ls s3://${S3_BUCKET}/10k-reports/ --recursive | grep ".metadata.json"
#+END_SRC

** Expected Enhancements

1. **Metadata Filtering**: Filter by year, document type, source
2. **Guardrails**: Content filtering, PII protection, topic blocking
3. **Reranking**: Improve relevance of retrieved chunks
4. **Citation Extraction**: Track sources in responses

** Validation Commands

#+BEGIN_SRC shell
# Test metadata filtering (year-based)
KB_ID="<your-kb-id>"  # From Lab 1
aws bedrock-agent-runtime retrieve \
  --knowledge-base-id ${KB_ID} \
  --retrieval-query '{"text": "Amazon revenue 2023"}' \
  --retrieval-configuration '{
    "vectorSearchConfiguration": {
      "filter": {
        "equals": {
          "key": "year",
          "value": "2023"
        }
      }
    }
  }' \
  --query "retrievalResults[:2].{content:content.text,metadata:metadata}" \
  --output json

# List guardrails
aws bedrock list-guardrails \
  --query "guardrails[*].{id:id,name:name,status:status}" \
  --output table

# Test generation with guardrails
GUARDRAIL_ID="<your-guardrail-id>"
aws bedrock-runtime invoke-model \
  --model-id anthropic.claude-3-haiku-20240307-v1:0 \
  --body '{"prompt": "Test prompt", "max_tokens": 100}' \
  --guardrail-identifier ${GUARDRAIL_ID} \
  --guardrail-version DRAFT \
  response.json
#+END_SRC

* Advanced RAG Patterns

Building on the basics, this module explores advanced techniques to improve RAG system performance, accuracy, and reliability.

** Key Topics
- Reranking for improved relevance
- Hybrid search combining keyword and semantic search
- Query expansion and reformulation
- RAG evaluation metrics

* Setup

#+BEGIN_SRC python
import os
import sys
sys.path.append('..')  # Add parent directory to path

import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import boto3
from dotenv import load_dotenv

load_dotenv()

# Advanced configuration
RERANK_TOP_K = int(os.getenv("RERANK_TOP_K", "20"))
FINAL_TOP_K = int(os.getenv("FINAL_TOP_K", "5"))
HYBRID_ALPHA = float(os.getenv("HYBRID_ALPHA", "0.7"))  # Weight for semantic search

print(f"Advanced RAG Configuration:")
print(f"  Rerank top-k: {RERANK_TOP_K}")
print(f"  Final top-k: {FINAL_TOP_K}")
print(f"  Hybrid search alpha: {HYBRID_ALPHA}")
#+END_SRC

* Reranking Implementation

Cross-encoder reranking significantly improves retrieval quality by considering the full context of query-document pairs.

#+BEGIN_SRC python
from src.agents.reranking import CrossEncoderReranker

# Create reranker
reranker = CrossEncoderReranker()

query = "What are the benefits of cloud computing?"
documents = [
    "Cloud computing offers scalability and cost savings.",
    "The weather forecast shows clouds tomorrow.",
    "AWS provides reliable cloud infrastructure.",
    "Cloud storage enables data backup and recovery.",
    "Cumulus clouds are fluffy and white."
]

results = reranker.rerank(query, documents, top_k=3)

print(f"Query: {query}\n")
print("Reranked results:")
for idx, score, doc in results:
    print(f"  Score: {score:.4f} - {doc}")
#+END_SRC

* Hybrid Search

Combining keyword-based (BM25) and semantic search often yields better results than either approach alone.

#+BEGIN_SRC python
from rank_bm25 import BM25Okapi
import numpy as np
from src.rag.embeddings import EmbeddingGenerator

# Demonstration of hybrid search using existing modules
# Sample documents
docs = [
    "Machine learning algorithms learn patterns from data.",
    "Deep learning uses neural networks for complex tasks.",
    "Natural language processing helps computers understand text.",
    "Computer vision enables machines to interpret images.",
    "Reinforcement learning trains agents through rewards."
]

# Generate embeddings
generator = EmbeddingGenerator()
doc_embeddings = generator.generate(docs)

# Initialize BM25
tokenized_docs = [doc.lower().split() for doc in docs]
bm25 = BM25Okapi(tokenized_docs)

# Hybrid search function
def hybrid_search(query, query_embedding, alpha=0.7, top_k=3):
    # BM25 search
    query_tokens = query.lower().split()
    bm25_scores = bm25.get_scores(query_tokens)
    
    # Normalize BM25 scores
    if max(bm25_scores) > 0:
        bm25_scores = bm25_scores / max(bm25_scores)
    
    # Semantic search (cosine similarity)
    semantic_scores = np.dot(doc_embeddings, query_embedding)
    semantic_scores = (semantic_scores + 1) / 2  # Normalize to [0, 1]
    
    # Combine scores
    final_scores = alpha * semantic_scores + (1 - alpha) * bm25_scores
    
    # Get top-k results
    top_indices = np.argsort(final_scores)[::-1][:top_k]
    
    results = []
    for idx in top_indices:
        results.append({
            'document': docs[idx],
            'score': float(final_scores[idx]),
            'bm25_score': float(bm25_scores[idx]),
            'semantic_score': float(semantic_scores[idx])
        })
    
    return results

# Search
query = "How do neural networks learn?"
query_emb = generator.generate(query)[0]

results = hybrid_search(query, query_emb)

print(f"Query: {query}\n")
for i, result in enumerate(results):
    print(f"{i+1}. {result['document']}")
    print(f"   Combined: {result['score']:.3f}, BM25: {result['bm25_score']:.3f}, Semantic: {result['semantic_score']:.3f}\n")
#+END_SRC

* Query Expansion

Improving retrieval by expanding queries with related terms and concepts.

#+BEGIN_SRC python
import spacy
from typing import List

# Simple query expansion demonstration
# Load spaCy model for NLP
try:
    nlp = spacy.load("en_core_web_sm")
except:
    print("spaCy model not loaded - install with: python -m spacy download en_core_web_sm")
    nlp = None

# Simple synonym dictionary (in practice, use WordNet or similar)
synonyms = {
    "machine learning": ["ML", "artificial intelligence", "AI", "deep learning"],
    "database": ["DB", "data store", "repository"],
    "cloud": ["cloud computing", "AWS", "Azure", "GCP"],
    "api": ["API", "interface", "endpoint", "service"],
    "rag": ["retrieval augmented generation", "retrieval-augmented generation"]
}

def expand_query(query: str, max_expansions: int = 5) -> List[str]:
    """Expand query with related terms."""
    expanded_queries = [query]
    query_lower = query.lower()
    
    # Check for known synonyms
    for term, syns in synonyms.items():
        if term in query_lower:
            for syn in syns[:max_expansions]:
                expanded = query_lower.replace(term, syn)
                if expanded not in expanded_queries:
                    expanded_queries.append(expanded)
    
    return expanded_queries[:max_expansions]

def generate_subqueries(query: str) -> List[str]:
    """Generate subqueries by decomposing complex queries."""
    subqueries = [query]
    
    # Extract questions from different aspects
    if "and" in query.lower():
        parts = query.lower().split("and")
        subqueries.extend([part.strip() for part in parts])
    
    # Use spaCy if available
    if nlp:
        doc = nlp(query)
        # Extract noun phrases as potential subqueries
        for chunk in doc.noun_chunks:
            if len(chunk.text.split()) > 1:
                subqueries.append(chunk.text)
    
    return list(set(subqueries))

# Example usage
queries = [
    "How does machine learning work in cloud environments?",
    "What is RAG and how is it used?",
    "Database API performance optimization"
]

for query in queries:
    print(f"\nOriginal: {query}")
    print("Expanded:")
    for expanded in expand_query(query):
        print(f"  - {expanded}")
    
    print("Subqueries:")
    for subquery in generate_subqueries(query):
        print(f"  - {subquery}")
#+END_SRC

* RAG Evaluation Metrics

Implementing metrics to evaluate RAG system performance.

#+BEGIN_SRC python
from typing import List, Dict
import numpy as np
from rouge import Rouge

# Simple evaluation metrics for RAG systems
def precision_at_k(retrieved: List[str], relevant: List[str], k: int) -> float:
    """Calculate Precision@K."""
    retrieved_k = retrieved[:k]
    relevant_set = set(relevant)
    
    hits = sum(1 for doc in retrieved_k if doc in relevant_set)
    return hits / k if k > 0 else 0.0

def recall_at_k(retrieved: List[str], relevant: List[str], k: int) -> float:
    """Calculate Recall@K."""
    retrieved_k = retrieved[:k]
    relevant_set = set(relevant)
    
    hits = sum(1 for doc in retrieved_k if doc in relevant_set)
    return hits / len(relevant) if len(relevant) > 0 else 0.0

def mean_reciprocal_rank(retrieved: List[str], relevant: List[str]) -> float:
    """Calculate Mean Reciprocal Rank (MRR)."""
    relevant_set = set(relevant)
    
    for i, doc in enumerate(retrieved):
        if doc in relevant_set:
            return 1.0 / (i + 1)
    return 0.0

def evaluate_generation(generated: str, reference: str) -> Dict[str, float]:
    """Evaluate generated text against reference."""
    results = {}
    
    # ROUGE scores
    try:
        rouge = Rouge()
        rouge_scores = rouge.get_scores(generated, reference)[0]
        results['rouge-1'] = rouge_scores['rouge-1']['f']
        results['rouge-2'] = rouge_scores['rouge-2']['f']
        results['rouge-l'] = rouge_scores['rouge-l']['f']
    except:
        results['rouge-1'] = 0.0
        results['rouge-2'] = 0.0
        results['rouge-l'] = 0.0
    
    return results

# Example evaluation
retrieved = ["doc1", "doc2", "doc3", "doc4", "doc5"]
relevant = ["doc2", "doc4", "doc6"]

print("Retrieval Metrics:")
print(f"Precision@5: {precision_at_k(retrieved, relevant, 5):.3f}")
print(f"Recall@5: {recall_at_k(retrieved, relevant, 5):.3f}")
print(f"MRR: {mean_reciprocal_rank(retrieved, relevant):.3f}")

# Generation evaluation
generated = "Cloud computing provides scalable infrastructure and reduces costs."
reference = "Cloud computing offers scalable resources and cost-effective solutions."

print("\nGeneration Metrics:")
gen_metrics = evaluate_generation(generated, reference)
for metric, score in gen_metrics.items():
    print(f"{metric}: {score:.3f}")
#+END_SRC

* Advanced RAG Pipeline

Combining all advanced techniques into an improved pipeline.

#+BEGIN_SRC python
from typing import List, Dict, Optional
from src.rag.embeddings import EmbeddingGenerator
from src.rag.vector_store import FAISSVectorStore
from src.rag.pipeline import RAGPipeline, RAGConfig
from src.agents.reranking import CrossEncoderReranker
import numpy as np
from rank_bm25 import BM25Okapi

# Demonstrate an advanced RAG pipeline with multiple techniques
class AdvancedRAGDemo:
    """Advanced RAG pipeline with reranking, hybrid search, and query expansion."""
    
    def __init__(self,
                 embedding_model: str = "all-MiniLM-L6-v2",
                 rerank_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
                 hybrid_alpha: float = 0.7):
        
        from embeddings import EmbeddingGenerator
        from vector_store import FAISSVectorStore
        
        self.embedder = EmbeddingGenerator(embedding_model)
        self.vector_store = FAISSVectorStore(self.embedder.dimension)
        self.reranker = CrossEncoderReranker(rerank_model)
        self.query_expander = QueryExpander()
        self.evaluator = RAGEvaluator()
        
        self.documents = []
        self.hybrid_alpha = hybrid_alpha
        self.hybrid_searcher = None
    
    def add_documents(self, documents: List[str], metadata: Optional[List[Dict]] = None):
        """Add documents to the pipeline."""
        # Store documents
        self.documents.extend(documents)
        
        # Generate embeddings
        embeddings = self.embedder.generate(documents)
        
        # Add to vector store
        self.vector_store.add(embeddings, documents, metadata)
        
        # Reinitialize hybrid searcher
        all_embeddings = self.embedder.generate(self.documents)
        self.hybrid_searcher = HybridSearcher(
            self.documents, 
            all_embeddings, 
            self.hybrid_alpha
        )
    
    def retrieve(self, 
                query: str, 
                use_query_expansion: bool = True,
                use_hybrid_search: bool = True,
                use_reranking: bool = True,
                initial_k: int = 20,
                final_k: int = 5) -> List[Dict]:
        """Advanced retrieval with all techniques."""
        
        # Query expansion
        if use_query_expansion:
            expanded_queries = self.query_expander.expand_query(query, max_expansions=3)
        else:
            expanded_queries = [query]
        
        all_results = []
        
        for exp_query in expanded_queries:
            query_embedding = self.embedder.generate(exp_query)[0]
            
            if use_hybrid_search and self.hybrid_searcher:
                # Hybrid search
                results = self.hybrid_searcher.search(
                    exp_query, 
                    query_embedding, 
                    top_k=initial_k
                )
                all_results.extend(results)
            else:
                # Standard semantic search
                results = self.vector_store.search(query_embedding, k=initial_k)
                all_results.extend(results)
        
        # Deduplicate results
        seen_docs = set()
        unique_results = []
        for result in all_results:
            doc_text = result.get('document', result.get('text', ''))
            if doc_text not in seen_docs:
                seen_docs.add(doc_text)
                unique_results.append(result)
        
        # Reranking
        if use_reranking and len(unique_results) > 0:
            documents = [r.get('document', r.get('text', '')) for r in unique_results]
            reranked = self.reranker.rerank(query, documents, top_k=final_k)
            
            # Update results with reranking scores
            final_results = []
            for idx, score, doc in reranked:
                result = unique_results[idx].copy()
                result['rerank_score'] = score
                final_results.append(result)
            
            return final_results
        else:
            return unique_results[:final_k]
    
    def _hybrid_search(self, query: str, query_embedding: np.ndarray, top_k: int) -> List[Dict]:
        """Perform hybrid BM25 + semantic search."""
        # BM25 search
        query_tokens = query.lower().split()
        bm25_scores = self.bm25.get_scores(query_tokens)
        
        # Get all embeddings for semantic scoring
        all_embeddings = self.embedder.generate(self.documents)
        
        # Semantic search (cosine similarity)
        semantic_scores = np.dot(all_embeddings, query_embedding)
        
        # Normalize scores
        if max(bm25_scores) > 0:
            bm25_scores = bm25_scores / max(bm25_scores)
        semantic_scores = (semantic_scores + 1) / 2
        
        # Combine scores
        final_scores = self.hybrid_alpha * semantic_scores + (1 - self.hybrid_alpha) * bm25_scores
        
        # Get top-k results
        top_indices = np.argsort(final_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'document': self.documents[idx],
                'score': float(final_scores[idx]),
                'index': int(idx)
            })
        
        return results

# Example usage
# Create advanced pipeline demo
pipeline = AdvancedRAGDemo()

# Add sample documents
documents = [
    "Machine learning models can be trained using supervised, unsupervised, or reinforcement learning approaches.",
    "Deep learning is a subset of machine learning that uses neural networks with multiple layers.",
    "Natural language processing enables computers to understand, interpret, and generate human language.",
    "Computer vision allows machines to interpret and understand visual information from the world.",
    "Transfer learning leverages pre-trained models to solve new but related problems efficiently.",
    "Federated learning enables training models on distributed data without centralizing it.",
    "Active learning selects the most informative samples for labeling to improve model performance.",
    "Meta-learning, or learning to learn, helps models adapt quickly to new tasks with minimal data."
]

pipeline.add_documents(documents)

# Test query
query = "How can machine learning models learn from limited data?"
print(f"Query: {query}")
print("-" * 50)

# Retrieve with all advanced techniques
results = pipeline.retrieve(
    query,
    use_query_expansion=True,
    use_hybrid_search=True,
    use_reranking=True,
    final_k=3
)

# Display results
print("Retrieved contexts:")
for i, result in enumerate(results, 1):
    doc = result.get('document', '')[:100]
    score = result.get('rerank_score', result.get('score', 0))
    print(f"{i}. (Score: {score:.3f}) {doc}...")

# Generate simple response
print(f"\nBased on the query '{query}', the most relevant findings are:")
for i, result in enumerate(results[:2], 1):
    print(f"{i}. {result.get('document', '')}")
#+END_SRC

* Exercises

** Exercise 1: Implement Custom Reranker
Create a reranker that uses multiple signals (semantic similarity, keyword overlap, entity matching).

** Exercise 2: Multi-Stage Retrieval
Implement a retrieval system with coarse-to-fine search: BM25 � Semantic � Reranking.

** Exercise 3: Query Understanding
Build a query classifier that determines query type (factual, analytical, comparative) and adjusts retrieval strategy.

** Exercise 4: Evaluation Dataset
Create a test dataset with queries, relevant documents, and reference answers to evaluate your RAG system.

* Lab 2 Validation Checklist

Complete these validation steps to ensure advanced RAG features are working:

** Metadata Filtering Validation

#+BEGIN_SRC shell
# 1. Test year-based filtering
KB_ID="<your-kb-id>"
for YEAR in 2021 2022 2023; do
  echo "=== Filtering for year $YEAR ==="
  aws bedrock-agent-runtime retrieve \
    --knowledge-base-id ${KB_ID} \
    --retrieval-query '{"text": "revenue"}' \
    --retrieval-configuration "{
      \"vectorSearchConfiguration\": {
        \"filter\": {\"equals\": {\"key\": \"year\", \"value\": \"$YEAR\"}}
      }
    }" \
    --query "length(retrievalResults)" \
    --output text
done

# 2. Test multiple filter conditions
aws bedrock-agent-runtime retrieve \
  --knowledge-base-id ${KB_ID} \
  --retrieval-query '{"text": "cloud services"}' \
  --retrieval-configuration '{
    "vectorSearchConfiguration": {
      "filter": {
        "andAll": [
          {"equals": {"key": "year", "value": "2023"}},
          {"equals": {"key": "document_type", "value": "10-K"}}
        ]
      }
    }
  }' \
  --query "retrievalResults[:1].metadata" \
  --output json
#+END_SRC

** Guardrails Testing

#+BEGIN_SRC shell
# 1. Test content filtering
echo '{
  "anthropic_version": "bedrock-2023-05-31",
  "max_tokens": 100,
  "messages": [{
    "role": "user",
    "content": "Tell me about Amazon financial performance"
  }]
}' > test-prompt.json

aws bedrock-runtime invoke-model \
  --model-id anthropic.claude-3-haiku-20240307-v1:0 \
  --body file://test-prompt.json \
  --guardrail-identifier ${GUARDRAIL_ID} \
  --guardrail-version DRAFT \
  --cli-binary-format raw-in-base64-out \
  response.json

# 2. Check if content was blocked
jq '.amazonBedrockGuardrailAction' response.json
#+END_SRC

** Citation Extraction

#+BEGIN_SRC shell
# Generate response with citations
aws bedrock-agent-runtime retrieve-and-generate \
  --input '{"text": "What was Amazon revenue in 2023?"}' \
  --retrieve-and-generate-configuration "{
    \"type\": \"KNOWLEDGE_BASE\",
    \"knowledgeBaseConfiguration\": {
      \"knowledgeBaseId\": \"${KB_ID}\",
      \"modelArn\": \"arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\"
    }
  }" \
  --query "citations[*].retrievedReferences[*].location.s3Location.uri" \
  --output text
#+END_SRC

** Performance Comparison

Capture these metrics for comparison:

1. **Without Filtering**:
   - Number of results retrieved
   - Relevance scores
   - Response time

2. **With Filtering**:
   - Number of results (should be fewer)
   - Relevance scores (should be higher)
   - Response time (may be slightly faster)

3. **With Guardrails**:
   - Blocked content instances
   - Modified responses
   - Processing overhead

** Expected Outputs

- [ ] Filtered queries returning only relevant year data
- [ ] Guardrails successfully blocking/modifying content
- [ ] Citations properly extracted with S3 locations
- [ ] Performance metrics showing improvement with filtering

Save these metrics for comparison with other labs.

* Summary

Advanced RAG techniques significantly improve system performance:

1. *Reranking*: Cross-encoders provide more accurate relevance scoring
2. *Hybrid Search*: Combining keyword and semantic search captures different aspects
3. *Query Expansion*: Related terms and subqueries improve recall
4. *Evaluation*: Systematic metrics help optimize the pipeline

Next module: [[file:03_text_to_sql.org][Text-to-SQL with Natural Language]]