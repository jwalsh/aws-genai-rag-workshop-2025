#+TITLE: Module 1: RAG Basics
#+AUTHOR: Jason Walsh
#+EMAIL: j@wal.sh
#+PROPERTY: header-args:python :results output :session rag-basics

* Introduction to RAG

Retrieval Augmented Generation (RAG) combines the power of large language models with external knowledge bases to provide more accurate, up-to-date, and verifiable responses.

** Key Components

1. *Document Processing*: Breaking down documents into manageable chunks
2. *Embeddings*: Converting text into vector representations
3. *Vector Storage*: Storing and indexing embeddings for fast retrieval
4. *Retrieval*: Finding relevant context for user queries
5. *Generation*: Using LLMs to synthesize responses

* Workshop Requirements - Lab 1: Build your RAG Application

** Learning Objectives
- Understand RAG fundamentals and chunking strategies
- Create vector stores and knowledge bases
- Implement different chunking methods (Fixed, Semantic, Hierarchical)
- Compare performance and costs of different approaches

** Prerequisites Validation

#+BEGIN_SRC shell
# 1. Verify AWS credentials are configured
aws sts get-caller-identity

# 2. Check Bedrock model access
aws bedrock list-foundation-models --region us-west-2 \
  --query "modelSummaries[?contains(modelId, 'titan-embed') || contains(modelId, 'claude')].modelId" \
  --output table

# 3. Verify required IAM permissions
aws iam get-role --role-name BedrockExecutionRoleForKB 2>/dev/null || echo "Role not found - needs creation"

# 4. Check S3 bucket naming (will be created later)
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
echo "S3 bucket will be: ${ACCOUNT_ID}-us-west-2-advanced-rag-workshop"
#+END_SRC

** Expected Resources to Create

1. S3 bucket for document storage
2. OpenSearch Serverless collection for vector storage
3. 4 Bedrock Knowledge Bases (one per chunking strategy)
4. CloudWatch log groups for monitoring

** Validation Commands

After setup, use these commands to verify resources:

#+BEGIN_SRC shell
# List S3 buckets (should see workshop bucket)
aws s3 ls | grep advanced-rag-workshop

# Check OpenSearch collections
aws opensearchserverless list-collections \
  --query "collectionSummaries[?name=='advancedrag'].{name:name, status:status}"

# List Knowledge Bases
aws bedrock-agent list-knowledge-bases \
  --query "knowledgeBaseSummaries[?contains(name, 'advanced-rag')].{name:name, status:status}"

# Verify document ingestion
aws s3 ls s3://${S3_BUCKET_NAME}/10k-reports/ --recursive
#+END_SRC

* Quick Setup

** 1. Verify Environment
#+BEGIN_SRC shell
# Check environment is activated (direnv should handle this)
which python
python --version

# Verify AWS CLI
aws --version
#+END_SRC

** 2. Start LocalStack (Optional)
#+BEGIN_SRC shell
# This starts LocalStack and creates all AWS resources automatically
make localstack-up
#+END_SRC

Note: LocalStack is only needed for AWS service integration examples. The basic RAG examples work without it.

* Sample Data Download

For our RAG pipeline demonstrations, we'll use Roget's Thesaurus as sample data.

** Download Using Make

#+BEGIN_SRC shell
# From the project root directory, run:
make download-data

# Or download just the thesaurus:
make data/rogets_thesaurus.pdf
#+END_SRC

The PDF will be downloaded to =data/rogets_thesaurus.pdf= in the project root.

For processing PDFs in our RAG pipeline, we'll use the PDF extractor from the project:

#+BEGIN_SRC python
import sys
sys.path.append('..')  # Add parent directory to path

from src.utils.pdf_extractor import PDFExtractor
import os

# Check if our sample PDF exists
pdf_path = "../data/rogets_thesaurus.pdf"

if os.path.exists(pdf_path):
    try:
        extractor = PDFExtractor()
        # Extract first 5 pages as a sample
        text = extractor.extract_text(pdf_path, max_pages=5)
        print(f"Extracted {len(text)} characters from first 5 pages")
        print("\nFirst 500 characters:")
        print(text[:500])
    except Exception as e:
        print(f"Error extracting PDF: {e}")
else:
    print(f"Sample PDF not found at {pdf_path}")
    print("Run 'make download-data' from the project root to download it.")
#+END_SRC

* Understanding RAG Components

** Document Chunking Demo

Chunking splits documents into manageable pieces for processing:

#+BEGIN_SRC python
from typing import List, Dict, Any

# Quick demo of chunking
from src.rag.chunking import SimpleChunker

sample_text = "RAG combines retrieval with generation. " * 20
chunker = SimpleChunker(chunk_size=100, overlap=20)
chunks = chunker.chunk_text(sample_text)

print(f"Text length: {len(sample_text)}")
print(f"Created {len(chunks)} chunks")
print(f"First chunk: {chunks[0]['text'][:50]}...")
print(f"Overlap demo: chunk 1 ends with: ...{chunks[0]['text'][-20:]}")
print(f"          chunk 2 starts with: {chunks[1]['text'][:20]}...")
#+END_SRC

** Embeddings with Bedrock

Generate embeddings using Amazon Titan:

#+BEGIN_SRC shell
# Create a sample embedding
echo '{"inputText": "What is machine learning?"}' > /tmp/embedding_request.json

aws bedrock-runtime invoke-model \
    --model-id amazon.titan-embed-text-v1 \
    --body file:///tmp/embedding_request.json \
    --cli-binary-format raw-in-base64-out \
    /tmp/embedding_response.json

# View embedding dimension
jq '.embedding | length' /tmp/embedding_response.json

# Show first 5 values
jq '.embedding[:5]' /tmp/embedding_response.json
#+END_SRC

** Compare with Local Embeddings

#+BEGIN_SRC python
from src.rag.embeddings import EmbeddingGenerator
import numpy as np

# Create embedding generator
generator = EmbeddingGenerator()

# Generate embeddings for sample texts
texts = [
    "What is machine learning?",
    "Machine learning is a subset of artificial intelligence.",
    "The weather is nice today."
]

embeddings = generator.generate(texts)
print(f"Embedding dimension: {generator.dimension}")
print(f"Generated {len(embeddings)} embeddings")

# Calculate similarities
for i in range(len(texts)):
    for j in range(i+1, len(texts)):
        # Calculate cosine similarity
        dot_product = np.dot(embeddings[i], embeddings[j])
        norm1 = np.linalg.norm(embeddings[i])
        norm2 = np.linalg.norm(embeddings[j])
        sim = dot_product / (norm1 * norm2)
        
        print(f"\nSimilarity between:")
        print(f"  '{texts[i]}'")
        print(f"  '{texts[j]}'")
        print(f"  Score: {sim:.4f}")
#+END_SRC

* Vector Storage with FAISS

Let's use the project's vector store implementation:

#+BEGIN_SRC python
from src.rag.vector_store import FAISSVectorStore
from src.rag.embeddings import EmbeddingGenerator

# Create vector store
generator = EmbeddingGenerator()
vector_store = FAISSVectorStore(dimension=generator.dimension)

# Add some documents
documents = [
    "Python is a high-level programming language.",
    "Machine learning enables computers to learn from data.",
    "Natural language processing deals with text analysis.",
    "Deep learning uses neural networks with multiple layers.",
    "AWS provides cloud computing services."
]

embeddings = generator.generate(documents)
vector_store.add(embeddings, documents)

# Search for similar documents
query = "What is artificial intelligence?"
query_embedding = generator.generate(query)

results = vector_store.search(query_embedding[0], k=3)

print(f"Query: {query}\n")
print("Top 3 similar documents:")
for result in results:
    print(f"\n- Document: {result['document']}")
    print(f"  Distance: {result['distance']:.4f}")
#+END_SRC

* Building a Simple RAG Pipeline

You can run a complete RAG pipeline demo using:
#+BEGIN_SRC shell
make run-rag-pipeline
#+END_SRC

Or build your own pipeline using the project's modules:

#+BEGIN_SRC python
from src.rag.pipeline import RAGPipeline, RAGConfig
from src.rag.chunking import SimpleChunker
from src.rag.embeddings import EmbeddingGenerator
from src.rag.vector_store import FAISSVectorStore

# Create a simple RAG demonstration
# First, let's use the individual components
chunker = SimpleChunker(chunk_size=512, overlap=50)
embedder = EmbeddingGenerator()
vector_store = FAISSVectorStore(dimension=embedder.dimension)

# Add some documents
documents = [
    """Amazon Web Services (AWS) is a subsidiary of Amazon that provides 
    on-demand cloud computing platforms and APIs to individuals, companies, 
    and governments, on a metered pay-as-you-go basis.""",
    
    """Machine learning is a subset of artificial intelligence that enables 
    systems to learn and improve from experience without being explicitly 
    programmed. It focuses on developing algorithms that can access data 
    and use it to learn for themselves.""",
    
    """Retrieval Augmented Generation (RAG) is a technique that combines 
    large language models with information retrieval systems. It allows 
    models to access external knowledge bases to provide more accurate 
    and up-to-date responses."""
]

# Process each document
for doc_id, doc in enumerate(documents):
    # Chunk the document
    chunks = chunker.chunk_text(doc)
    chunk_texts = [chunk['text'] for chunk in chunks]
    
    # Generate embeddings
    embeddings = embedder.generate(chunk_texts)
    
    # Add to vector store with metadata
    metadata = [{'doc_id': doc_id, 'chunk_index': i} for i in range(len(chunks))]
    vector_store.add(embeddings, chunk_texts, metadata)

# Test queries
queries = [
    "What is AWS?",
    "Explain machine learning",
    "How does RAG work?"
]

for query in queries:
    print("="*50)
    print(f"Query: {query}\n")
    
    # Generate query embedding
    query_embedding = embedder.generate(query)[0]
    
    # Search for similar documents
    results = vector_store.search(query_embedding, k=2)
    
    print("Retrieved contexts:")
    for i, result in enumerate(results):
        print(f"\n{i+1}. {result['document'][:100]}...")
        print(f"   (Distance: {result['distance']:.4f})")
#+END_SRC

* Exercises

** Exercise 1: Implement Semantic Chunking
Modify the chunker to split on sentence boundaries instead of fixed character counts.

** Exercise 2: Add Metadata Filtering
Enhance the vector store to filter results based on metadata before returning.

** Exercise 3: Integrate with AWS Bedrock
Replace the local embedding model with Amazon Bedrock's Titan Embeddings.

* AWS Integration with LocalStack and Bedrock

Let's demonstrate AWS integration using the project's utilities:

#+BEGIN_SRC python
from src.utils.aws_client import get_bedrock_runtime_client, get_s3_client
from src.rag.embeddings import EmbeddingGenerator
from src.rag.chunking import SimpleChunker
from src.rag.vector_store import FAISSVectorStore
import json
import os

# Set up for LocalStack
os.environ['LOCALSTACK_ENDPOINT'] = 'http://localhost:4566'

# Get AWS clients
bedrock = get_bedrock_runtime_client()
s3 = get_s3_client()

# Initialize components
chunker = SimpleChunker(chunk_size=512, overlap=50)
embedder = EmbeddingGenerator()
vector_store = FAISSVectorStore(dimension=embedder.dimension)

# Cost tracking
costs = {
    'embedding_requests': 0,
    'llm_requests': 0,
    'storage_operations': 0
}

def generate_bedrock_embeddings(texts):
    """Generate embeddings using Amazon Bedrock Titan."""
    embeddings = []
    
    for text in texts:
        try:
            response = bedrock.invoke_model(
                modelId="amazon.titan-embed-text-v1",
                body=json.dumps({"inputText": text})
            )
            
            result = json.loads(response['body'].read())
            embeddings.append(result['embedding'])
            
            # Track costs (Titan Embeddings: $0.0001 per 1K tokens)
            estimated_tokens = len(text.split()) * 1.3  # Rough estimation
            costs['embedding_requests'] += (estimated_tokens / 1000) * 0.0001
            
        except Exception as e:
            print(f"Bedrock embedding failed, falling back to local: {e}")
            # Fallback to local embeddings
            local_emb = embedder.generate([text])[0]
            embeddings.append(local_emb.tolist())
    
    return embeddings

# Test with sample text
sample_text = "Amazon Web Services provides cloud computing services."

# Test embedding generation
embeddings = generate_bedrock_embeddings([sample_text])
print(f"Generated {len(embeddings)} embeddings")
if embeddings:
    print(f"Embedding dimension: {len(embeddings[0])}")

# Test S3 storage
bucket = "workshop-rag-documents"
try:
    s3.put_object(
        Bucket=bucket,
        Key="test-doc.txt",
        Body=sample_text.encode()
    )
    costs['storage_operations'] += 1
    print(f"Stored in: s3://{bucket}/test-doc.txt")
except Exception as e:
    print(f"S3 storage failed: {e}")

# Calculate costs
total_cost = (
    costs['embedding_requests'] +  # Titan Embeddings
    costs['llm_requests'] * 0.003 +  # Claude 3 Haiku estimate
    costs['storage_operations'] * 0.0004  # S3 PUT requests
)

print(f"Estimated costs: ${total_cost:.6f}")
#+END_SRC

* Cost Analysis

Understanding AWS costs is crucial for production RAG systems.

#+BEGIN_SRC python
from src.utils.cost_calculator import RAGCostEstimator

# Create cost estimator
estimator = RAGCostEstimator()

# Example: Small business use case
monthly_costs = estimator.estimate_monthly_cost(
    documents_per_month=1000,    # 1K new documents
    queries_per_month=10000,     # 10K queries
    storage_gb=5.0               # 5GB storage
)

print("Monthly Cost Estimate:")
print(f"  Embeddings: ${monthly_costs['embedding_cost']}")
print(f"  Queries: ${monthly_costs['query_cost']}")
print(f"  Storage: ${monthly_costs['storage_cost']}")
print(f"  Total: ${monthly_costs['total_monthly_cost']}")

# Medium enterprise example
enterprise_costs = estimator.estimate_monthly_cost(
    documents_per_month=10000,
    queries_per_month=100000,
    storage_gb=50.0
)

print("\nEnterprise Cost Estimate:")
print(f"  Total: ${enterprise_costs['total_monthly_cost']}")
#+END_SRC

* Integration with Project Modules

Let's demonstrate using the full project pipeline:

#+BEGIN_SRC python
from src.rag.pipeline import RAGPipeline, RAGConfig
from src.utils.aws_client import get_bedrock_runtime_client
import os

# Use project's RAG configuration
config = RAGConfig(
    chunk_size=512,
    chunk_overlap=50,
    embedding_model="amazon.titan-embed-text-v1",
    retrieval_k=5
)

# Create production pipeline
try:
    production_pipeline = RAGPipeline(config)
    print("Production RAG pipeline created successfully")
except Exception as e:
    print(f"Pipeline creation error (expected with LocalStack): {e}")

# Test document
test_doc = """
Retrieval Augmented Generation (RAG) combines large language models 
with external knowledge bases. This approach enables more accurate, 
up-to-date, and verifiable responses by retrieving relevant information 
before generating answers.
"""

# Show integration capabilities
print("\nProject Integration Features:")
print("- Production RAG pipeline available")
print("- AWS client utilities configured")
print("- Cost calculation utilities")
print("- Modular component architecture")
print("- Guardrails and safety checks")
print("- SQL agent capabilities")

# List available modules
print("\nAvailable src modules:")
for module in ['rag', 'agents', 'guardrails', 'utils']:
    print(f"- src.{module}")
#+END_SRC

* Level 1 Test: Philosophical RAG Demo

This test demonstrates a Python-only RAG system using philosophical texts. It requires no AWS services or Docker, making it perfect for testing basic RAG functionality.

** Download Philosophy Texts

First, download the philosophical texts we'll use:

#+BEGIN_SRC shell
# Download all philosophy texts (50MB+)
make download-all

# Or download individually:
make download-philosophy
#+END_SRC

** Run Philosophical RAG Test

This test queries across Boethius, Kant, Wittgenstein, and Roget's Thesaurus:

#+BEGIN_SRC python
from src.demos.philosophical_rag import PhilosophicalRAG

# Create philosophical RAG system
rag = PhilosophicalRAG()

# Load texts (this may take a moment)
print("Loading philosophical texts...")
rag.load_texts()

# Test query that should hit multiple sources
query = "What is the relationship between language, meaning, and synonyms?"
print(f"\nQuerying: {query}")
rag.philosophical_query(query, n_results=6)

# Test incuriosity concept across texts
query2 = "What is incuriosity, indifference, or the absence of interest?"
print(f"\n\nQuerying: {query2}")
rag.philosophical_query(query2, n_results=6)

# Fun workshop demo: Ideal software developer characteristics
print("\n\n🎯 AWS Workshop Special: What makes a great developer?")
print("="*60)

# Query for developer qualities using philosophical language
dev_query = "What are the qualities of demonstration, proof, experimentation, curiosity, and practical wisdom in creating artifacts?"
print(f"\nQuerying: {dev_query}")
rag.philosophical_query(dev_query, n_results=6)

# Engineering mindset
eng_query = "What is the nature of methodical investigation, systematic inquiry, and the art of construction?"
print(f"\n\nQuerying: {eng_query}")
rag.philosophical_query(eng_query, n_results=6)
#+END_SRC

** Expected Results

You should see:
1. **Cross-text connections**: Results from different philosophers
2. **Thesaurus bridges**: Roget's entries connecting concepts
3. **Relevance scores**: How well each chunk matches the query

This demonstrates RAG working with:
- Dense philosophical language
- Cross-era terminology
- Concept mapping via thesaurus
- No external dependencies (Python-only)

** Fun Workshop Demo: Developer Characteristics via Philosophy

The developer characteristics queries show how RAG can find amusing connections between:
- Roget's formal categorizations (demonstration, proof, experimentation)
- Kant's systematic investigation and architectural thinking
- Wittgenstein's focus on language and meaning construction
- Boethius on wisdom and practical knowledge

Perfect for showing workshop attendees that ideal developers should have:
- **Curiosity** (vs. incuriosity/indifference from Roget entry 458)
- **Perspicuity** (lucidity, clearness, intelligibility from Roget entry 570)
- **Systematic thinking** (Kant's "articulation or organization")
- **Experimental mindset** (Kant's empirical experimentation)
- **Focus on demonstration** (proof, conclusiveness, apodixis from Roget)
- **Practical wisdom** (Boethius on applied knowledge)

As Roget entry 570 states: "perspicuous, clear, lucid, intelligible" - exactly what we want in code, documentation, and literate programming notebooks!

* Lab 1 Validation Checklist

Complete these validation steps to ensure your RAG system is properly configured:

** Resource Creation Validation

#+BEGIN_SRC shell
# 1. Verify S3 bucket exists with documents
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
S3_BUCKET="${ACCOUNT_ID}-us-west-2-advanced-rag-workshop"

aws s3 ls s3://${S3_BUCKET}/10k-reports/ | head -5

# 2. Check OpenSearch collection status
aws opensearchserverless list-collections \
  --query "collectionSummaries[?contains(name, 'advancedrag')]"

# 3. List all Knowledge Bases created
aws bedrock-agent list-knowledge-bases \
  --query "knowledgeBaseSummaries[*].{id:knowledgeBaseId,name:name,status:status}" \
  --output table

# 4. Test retrieval from each Knowledge Base
KB_ID="<your-kb-id>"  # Replace with actual ID
aws bedrock-agent-runtime retrieve \
  --knowledge-base-id ${KB_ID} \
  --retrieval-query '{"text": "What was Amazon revenue in 2023?"}' \
  --query "retrievalResults[:2].content.text" \
  --output text
#+END_SRC

** Performance Metrics to Capture

1. **Ingestion Metrics**:
   - Time to ingest documents
   - Number of chunks created per strategy
   - Token count for embeddings

2. **Retrieval Quality**:
   - Relevance scores for test queries
   - Response latency
   - Chunk overlap analysis

3. **Cost Analysis**:
   - Embedding generation costs
   - Storage costs (OpenSearch)
   - Query costs

** Expected Outputs

After completing Lab 1, you should have:

- [ ] 4 Knowledge Base IDs saved to configuration
- [ ] Cost comparison table showing token usage per chunking strategy
- [ ] Sample retrieval results from each KB
- [ ] Performance metrics for each chunking approach

Save these for comparison in Lab 4 (FloTorch evaluation).

* Summary

In this module, we've built a comprehensive RAG system that includes:

1. **Core Components**: Document chunking, embedding generation, vector storage
2. **AWS Integration**: LocalStack testing and Bedrock integration
3. **Cost Analysis**: Detailed cost estimation for production use
4. **Project Integration**: Connection with existing project modules
5. **Production Ready**: Error handling, configuration, and monitoring

**Key Learning Outcomes:**
- Understanding RAG fundamentals and implementation
- AWS service integration (Bedrock, S3, DynamoDB)
- Cost optimization strategies
- Production deployment considerations

**Cost Estimates for Common Use Cases:**
- Small business (1K docs, 10K queries/month): ~$3.68/month
- Medium enterprise (10K docs, 100K queries/month): ~$36.80/month
- Large scale (100K docs, 1M queries/month): ~$368/month

Next module: [[file:02_advanced_rag.org][Advanced RAG Techniques]]