#+TITLE: Module 1: RAG Basics
#+AUTHOR: Jason Walsh
#+EMAIL: j@wal.sh
#+DATE: 2025-05-30
#+PROPERTY: header-args:python :results output :session rag-basics
#+PROPERTY: header-args:shell :results output

* Workshop Overview & Weekend Testing Plan

This notebook provides a comprehensive guide for building RAG applications, from local testing to full AWS integration.

** Weekend Testing Checklist

- [ ] AWS credentials working (source .env)
- [ ] Bedrock models accessible
- [ ] S3 bucket creation successful
- [ ] OpenSearch collection setup
- [ ] 4 Knowledge Bases created (different chunking strategies)
- [ ] Retrieval working from each KB
- [ ] Cost tracking accurate
- [ ] Local RAG demo working (no AWS required)

* Workshop Requirements - Lab 1: Build your RAG Application

** Learning Objectives
- Understand RAG fundamentals and chunking strategies
- Create vector stores and knowledge bases
- Implement different chunking methods (Fixed, Semantic, Hierarchical)
- Compare performance and costs of different approaches

** Prerequisites Validation

#+BEGIN_SRC shell :dir /Users/jasonwalsh/projects/jwalsh/aws-genai-rag-workshop-2025
# Source credentials
source .env

# 1. Verify AWS credentials are configured
aws sts get-caller-identity

# 2. Check Bedrock model access
aws bedrock list-foundation-models --region us-west-2 \
  --query "modelSummaries[?contains(modelId, 'titan-embed') || contains(modelId, 'claude')].modelId" \
  --output table

# 3. Verify required IAM permissions
aws iam get-role --role-name BedrockExecutionRoleForKB 2>/dev/null || echo "Role not found - needs creation"

# 4. Check S3 bucket naming (will be created later)
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
echo "S3 bucket will be: ${ACCOUNT_ID}-us-west-2-advanced-rag-workshop"
#+END_SRC

** Resource Setup Commands

#+BEGIN_SRC shell :dir /Users/jasonwalsh/projects/jwalsh/aws-genai-rag-workshop-2025
# Source environment and setup script
source .env
source live/debug.org  # Contains get_resource_names function

# Get resource names
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
REGION_NAME="us-west-2"

# Set all resource names
S3_BUCKET_NAME="${ACCOUNT_ID}-${REGION_NAME}-advanced-rag-workshop"
KNOWLEDGE_BASE_NAME_AOSS="advanced-rag-workshop-knowledgebase-aoss"
OSS_VECTOR_STORE_NAME="advancedrag"
OSS_INDEX_NAME="ws-index-"

# Create S3 bucket
aws s3 mb s3://${S3_BUCKET_NAME} --region ${REGION_NAME} || echo "Bucket already exists"

# Download 10-K reports
mkdir -p /tmp/10k-reports
cd /tmp/10k-reports

# Download Amazon 10-K reports
wget -q "https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/e42c2068-bad5-4ab6-ae57-36ff8b2aeffd.pdf" -O Amazon_10K_2023.pdf
wget -q "https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf" -O Amazon_10K_2022.pdf
wget -q "https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf" -O Amazon_10K_2021.pdf

# Create metadata files
echo '{"company": "Amazon", "year": "2023", "document_type": "10-K"}' > Amazon_10K_2023.pdf.metadata.json
echo '{"company": "Amazon", "year": "2022", "document_type": "10-K"}' > Amazon_10K_2022.pdf.metadata.json
echo '{"company": "Amazon", "year": "2021", "document_type": "10-K"}' > Amazon_10K_2021.pdf.metadata.json

# Upload to S3
aws s3 sync . s3://${S3_BUCKET_NAME}/10k-reports/

cd -
#+END_SRC

* Introduction to RAG

Retrieval Augmented Generation (RAG) combines the power of large language models with external knowledge bases to provide more accurate, up-to-date, and verifiable responses.

** Key Components

1. *Document Processing*: Breaking down documents into manageable chunks
2. *Embeddings*: Converting text into vector representations
3. *Vector Storage*: Storing and indexing embeddings for fast retrieval
4. *Retrieval*: Finding relevant context for user queries
5. *Generation*: Using LLMs to synthesize responses

* Quick Setup & Environment Check

#+BEGIN_SRC shell
# Check environment is activated
which python
python --version

# Verify AWS CLI
aws --version

# Check if workshop modules are available
python -c "import sys; sys.path.append('..'); from src.rag.pipeline import RAGPipeline; print('✓ RAG modules available')"
#+END_SRC

* Level 0 Test: Basic Embeddings (No AWS Required)

Let's start with a simple test that requires no AWS services:

#+BEGIN_SRC python
import sys
sys.path.append('..')  # Add parent directory to path

from src.rag.embeddings import EmbeddingGenerator
from src.rag.vector_store import FAISSVectorStore
import numpy as np

# Create local components
generator = EmbeddingGenerator()
vector_store = FAISSVectorStore(dimension=generator.dimension)

# Test documents
documents = [
    "RAG combines retrieval with generation for better AI responses.",
    "Vector databases store embeddings for similarity search.",
    "AWS Bedrock provides managed foundation models.",
    "Knowledge bases enable semantic search over documents."
]

# Generate embeddings
embeddings = generator.generate(documents)
vector_store.add(embeddings, documents)

# Test retrieval
query = "How does RAG work?"
query_embedding = generator.generate(query)[0]
results = vector_store.search(query_embedding, k=2)

print(f"Query: {query}\n")
for i, result in enumerate(results):
    print(f"{i+1}. {result['document']}")
    print(f"   Distance: {result['distance']:.4f}\n")
#+END_SRC

* Level 1 Test: Simple RAG Pipeline (No AWS Required)

** Option A: Philosophical RAG Demo

This demonstrates RAG with philosophy texts - requires no AWS services:

#+BEGIN_SRC shell
# Download philosophy texts if not already present
make download-philosophy
#+END_SRC

#+BEGIN_SRC python
from src.demos.philosophical_rag import PhilosophicalRAG

# Create philosophical RAG system
rag = PhilosophicalRAG()

# Load texts (this may take a moment)
print("Loading philosophical texts...")
rag.load_texts()

# Test queries
queries = [
    "What is the relationship between language and meaning?",
    "What is wisdom according to ancient philosophy?",
    "How do we gain knowledge through experience?"
]

for query in queries:
    print(f"\n{'='*60}")
    print(f"Query: {query}")
    print('='*60)
    rag.philosophical_query(query, n_results=3)
#+END_SRC

** Option B: Minimal RAG Validation (Download/Chunk/Pickle)

Simple RAG pipeline that can run anywhere (CI/CD friendly):

#+BEGIN_SRC python
import os
import pickle
import requests
from pathlib import Path
import numpy as np
from typing import List, Dict

# Create simple RAG components
class SimpleRAG:
    def __init__(self, cache_dir="./rag_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
    def download_sample_text(self) -> str:
        """Download a sample text file"""
        cache_file = self.cache_dir / "sample_text.txt"
        
        if cache_file.exists():
            print("✓ Using cached text")
            return cache_file.read_text()
        
        print("→ Downloading sample text...")
        url = "https://www.gutenberg.org/files/74/74-0.txt"  # Tom Sawyer
        response = requests.get(url)
        text = response.text
        
        # Cache it
        cache_file.write_text(text)
        print("✓ Downloaded and cached")
        return text
    
    def chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """Simple chunking by character count"""
        print(f"→ Chunking text (size={chunk_size})...")
        chunks = []
        words = text.split()
        current_chunk = []
        current_size = 0
        
        for word in words:
            current_chunk.append(word)
            current_size += len(word) + 1
            
            if current_size >= chunk_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_size = 0
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        print(f"✓ Created {len(chunks)} chunks")
        return chunks
    
    def create_embeddings(self, chunks: List[str]) -> np.ndarray:
        """Create simple embeddings (hash-based for demo)"""
        print("→ Creating embeddings...")
        embeddings = []
        
        for chunk in chunks:
            # Simple hash-based embedding (not ML, but deterministic)
            words = chunk.lower().split()
            embedding = np.zeros(128)  # 128-dim embedding
            
            for word in words:
                # Simple hash to position
                positions = [hash(word + str(i)) % 128 for i in range(3)]
                for pos in positions:
                    embedding[pos] += 1.0
            
            # Normalize
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
                
            embeddings.append(embedding)
        
        embeddings = np.array(embeddings)
        print(f"✓ Created embeddings with shape {embeddings.shape}")
        return embeddings
    
    def pickle_index(self, chunks: List[str], embeddings: np.ndarray) -> str:
        """Pickle the RAG index"""
        index_file = self.cache_dir / "rag_index.pkl"
        
        index = {
            'chunks': chunks,
            'embeddings': embeddings,
            'metadata': {
                'chunk_size': 500,
                'num_chunks': len(chunks),
                'embedding_dim': embeddings.shape[1]
            }
        }
        
        with open(index_file, 'wb') as f:
            pickle.dump(index, f)
        
        print(f"✓ Pickled index to {index_file}")
        return str(index_file)
    
    def search(self, query: str, index_file: str, top_k: int = 3) -> List[Dict]:
        """Search the RAG index"""
        # Load index
        with open(index_file, 'rb') as f:
            index = pickle.load(f)
        
        # Create query embedding
        query_embedding = self.create_embeddings([query])[0]
        
        # Calculate similarities
        similarities = np.dot(index['embeddings'], query_embedding)
        
        # Get top results
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append({
                'chunk': index['chunks'][idx][:200] + '...',
                'score': similarities[idx],
                'index': idx
            })
        
        return results

# Run the validation
print("=== Level 1 RAG Validation ===\n")

rag = SimpleRAG()

# Step 1: Download
text = rag.download_sample_text()
print(f"   Text length: {len(text)} chars")

# Step 2: Chunk
chunks = rag.chunk_text(text)

# Step 3: Create embeddings
embeddings = rag.create_embeddings(chunks)

# Step 4: Pickle
index_file = rag.pickle_index(chunks, embeddings)

# Step 5: Test search
print("\n→ Testing search...")
test_queries = [
    "What did Tom Sawyer do?",
    "Who is Huckleberry Finn?",
    "Tell me about the fence painting."
]

for query in test_queries:
    print(f"\nQuery: {query}")
    results = rag.search(query, index_file, top_k=2)
    for i, result in enumerate(results):
        print(f"{i+1}. Score: {result['score']:.3f}")
        print(f"   {result['chunk']}\n")

print("✓ Level 1 validation complete!")
#+END_SRC

** Platform-Specific Notes (Unverified)

*** GitHub Actions
#+BEGIN_SRC yaml
# .github/workflows/rag-validation.yml
name: RAG Level 1 Validation

on: [push, pull_request]

jobs:
  validate-rag:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install numpy requests
    
    - name: Run Level 1 RAG validation
      run: |
        python -c "
        # Copy the SimpleRAG code here
        # Or create scripts/validate_rag_level1.py
        "
    
    - name: Upload RAG artifacts
      uses: actions/upload-artifact@v3
      with:
        name: rag-index
        path: rag_cache/
#+END_SRC

*** GitHub Codespaces
#+BEGIN_SRC json
// .devcontainer/devcontainer.json
{
  "name": "RAG Workshop",
  "image": "mcr.microsoft.com/devcontainers/python:3.11",
  "features": {
    "ghcr.io/devcontainers/features/aws-cli:1": {}
  },
  "postCreateCommand": "pip install -r requirements.txt && make download-philosophy",
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-toolsai.jupyter"
      ]
    }
  }
}
#+END_SRC

*** Jupyter on AWS (SageMaker/EMR)
#+BEGIN_SRC python
# First cell in Jupyter notebook
!pip install numpy requests faiss-cpu

# Run validation in notebook
import sys
sys.path.append('/home/sagemaker-user')  # Adjust for your environment

# Then run the SimpleRAG code above
#+END_SRC

* AWS Integration: Bedrock Embeddings

Now let's test AWS Bedrock integration:

#+BEGIN_SRC shell
# Test Bedrock embedding
echo '{"inputText": "What is machine learning?"}' > /tmp/embedding_request.json

aws bedrock-runtime invoke-model \
    --model-id amazon.titan-embed-text-v1 \
    --body file:///tmp/embedding_request.json \
    --cli-binary-format raw-in-base64-out \
    /tmp/embedding_response.json \
    --region us-west-2

# Check embedding dimension
jq '.embedding | length' /tmp/embedding_response.json
#+END_SRC

* Creating Knowledge Bases with Different Chunking Strategies

** 1. Create OpenSearch Collection

#+BEGIN_SRC shell
# Create OpenSearch policies
aws opensearchserverless create-security-policy \
  --name "${OSS_VECTOR_STORE_NAME}-encryption" \
  --type encryption \
  --policy "{\"Rules\":[{\"ResourceType\":\"collection\",\"Resource\":[\"collection/${OSS_VECTOR_STORE_NAME}\"]}],\"AWSOwnedKey\":true}" \
  --region us-west-2

aws opensearchserverless create-security-policy \
  --name "${OSS_VECTOR_STORE_NAME}-network" \
  --type network \
  --policy "[{\"Rules\":[{\"ResourceType\":\"collection\",\"Resource\":[\"collection/${OSS_VECTOR_STORE_NAME}\"]}],\"AllowFromPublic\":true}]" \
  --region us-west-2

# Create collection
aws opensearchserverless create-collection \
  --name ${OSS_VECTOR_STORE_NAME} \
  --type VECTORSEARCH \
  --region us-west-2
#+END_SRC

** 2. Create IAM Role for Bedrock

#+BEGIN_SRC shell
# Create trust policy
cat > /tmp/bedrock-trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "bedrock.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

# Create role (if not exists)
aws iam create-role \
  --role-name BedrockExecutionRoleForKB \
  --assume-role-policy-document file:///tmp/bedrock-trust-policy.json \
  2>/dev/null || echo "Role already exists"

# Attach policies
aws iam attach-role-policy \
  --role-name BedrockExecutionRoleForKB \
  --policy-arn arn:aws:iam::aws:policy/AmazonBedrockFullAccess

aws iam attach-role-policy \
  --role-name BedrockExecutionRoleForKB \
  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
#+END_SRC

** 3. Create Knowledge Bases

We'll create 4 KBs with different chunking strategies:

#+BEGIN_SRC python
# This would normally use the Python SDK, but for validation we'll use AWS CLI
chunking_configs = [
    {
        "name": "fixed-chunking",
        "strategy": "FIXED_SIZE",
        "config": {"maxTokens": 300, "overlapPercentage": 10}
    },
    {
        "name": "semantic-chunking", 
        "strategy": "SEMANTIC",
        "config": {"maxTokens": 300, "bufferSize": 0, "breakpointPercentileThreshold": 95}
    },
    {
        "name": "hierarchical-chunking",
        "strategy": "HIERARCHICAL",
        "config": {"levelConfigurations": [{"maxTokens": 1500}, {"maxTokens": 300}], "overlapTokens": 60}
    },
    {
        "name": "no-chunking",
        "strategy": "NONE",
        "config": {}
    }
]

print("Knowledge Base configurations prepared:")
for config in chunking_configs:
    print(f"- {config['name']}: {config['strategy']}")
#+END_SRC

* Testing Retrieval

After KBs are created, test retrieval:

#+BEGIN_SRC shell
# Test retrieval from a Knowledge Base
KB_ID="<your-kb-id>"  # Replace with actual KB ID

aws bedrock-agent-runtime retrieve \
  --knowledge-base-id ${KB_ID} \
  --retrieval-query '{"text": "What was Amazon revenue in 2023?"}' \
  --retrieval-configuration '{"vectorSearchConfiguration": {"numberOfResults": 3}}' \
  --region us-west-2 \
  --output json | jq '.retrievalResults[0].content.text'
#+END_SRC

* Cost Analysis

#+BEGIN_SRC python
from src.utils.cost_calculator import RAGCostEstimator

# Create cost estimator
estimator = RAGCostEstimator()

# Estimate costs for the workshop
workshop_costs = estimator.estimate_monthly_cost(
    documents_per_month=10,      # 10-K reports
    queries_per_month=100,       # Testing queries
    storage_gb=0.1               # ~100MB of PDFs
)

print("Workshop Cost Estimate:")
print(f"  Embeddings: ${workshop_costs['embedding_cost']:.2f}")
print(f"  Queries: ${workshop_costs['query_cost']:.2f}")
print(f"  Storage: ${workshop_costs['storage_cost']:.2f}")
print(f"  Total: ${workshop_costs['total_monthly_cost']:.2f}")
#+END_SRC

* Lab 1 Validation Checklist

Complete these validation steps to ensure your RAG system is properly configured:

** Resource Creation Validation

#+BEGIN_SRC shell
# 1. Verify S3 bucket exists with documents
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
S3_BUCKET="${ACCOUNT_ID}-us-west-2-advanced-rag-workshop"

aws s3 ls s3://${S3_BUCKET}/10k-reports/ | head -5

# 2. Check OpenSearch collection status
aws opensearchserverless list-collections \
  --query "collectionSummaries[?contains(name, 'advancedrag')]"

# 3. List all Knowledge Bases created
aws bedrock-agent list-knowledge-bases \
  --query "knowledgeBaseSummaries[*].{id:knowledgeBaseId,name:name,status:status}" \
  --output table

# 4. Test retrieval from each Knowledge Base
KB_ID="<your-kb-id>"  # Replace with actual ID
aws bedrock-agent-runtime retrieve \
  --knowledge-base-id ${KB_ID} \
  --retrieval-query '{"text": "What was Amazon revenue in 2023?"}' \
  --query "retrievalResults[:2].content.text" \
  --output text
#+END_SRC

** Performance Metrics to Capture

1. **Ingestion Metrics**:
   - Time to ingest documents
   - Number of chunks created per strategy
   - Token count for embeddings

2. **Retrieval Quality**:
   - Relevance scores for test queries
   - Response latency
   - Chunk overlap analysis

3. **Cost Analysis**:
   - Embedding generation costs
   - Storage costs (OpenSearch)
   - Query costs

** Expected Outputs

After completing Lab 1, you should have:

- [ ] 4 Knowledge Base IDs saved to configuration
- [ ] Cost comparison table showing token usage per chunking strategy
- [ ] Sample retrieval results from each KB
- [ ] Performance metrics for each chunking approach

Save these for comparison in Lab 4 (FloTorch evaluation).

* Weekend Testing Script

Save this script to quickly validate the entire setup:

#+BEGIN_SRC shell :tangle weekend-test.sh
#!/bin/bash
# Weekend Workshop Validation Script

echo "=== AWS GenAI RAG Workshop Validation ==="
echo "Date: $(date)"
echo ""

# Source credentials
source .env

# Check AWS access
echo "1. Checking AWS credentials..."
aws sts get-caller-identity || { echo "❌ AWS credentials not working"; exit 1; }
echo "✅ AWS credentials OK"

# Check Bedrock models
echo -e "\n2. Checking Bedrock model access..."
MODEL_COUNT=$(aws bedrock list-foundation-models --region us-west-2 --query 'length(modelSummaries)' --output text)
echo "✅ Found $MODEL_COUNT Bedrock models"

# Check S3 bucket
echo -e "\n3. Checking S3 bucket..."
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
S3_BUCKET="${ACCOUNT_ID}-us-west-2-advanced-rag-workshop"
aws s3 ls s3://${S3_BUCKET}/ && echo "✅ S3 bucket exists" || echo "❌ S3 bucket not found"

# Check OpenSearch
echo -e "\n4. Checking OpenSearch collections..."
aws opensearchserverless list-collections --query 'length(collectionSummaries)' --output text

# Check Knowledge Bases
echo -e "\n5. Checking Knowledge Bases..."
KB_COUNT=$(aws bedrock-agent list-knowledge-bases --query 'length(knowledgeBaseSummaries)' --output text)
echo "Found $KB_COUNT Knowledge Bases"

# Test local RAG
echo -e "\n6. Testing local RAG (no AWS)..."
python -c "
import sys
sys.path.append('..')
from src.rag.embeddings import EmbeddingGenerator
print('✅ Local RAG components working')
"

echo -e "\n=== Validation Complete ==="
#+END_SRC

* Troubleshooting Guide

** Common Issues and Solutions

1. **AWS Credentials Error**
   #+BEGIN_SRC shell
   # Make sure .env is sourced
   source .env
   aws sts get-caller-identity
   #+END_SRC

2. **Bedrock Model Access Denied**
   - Check region is us-west-2
   - Verify models are enabled in Bedrock console

3. **S3 Bucket Name Conflict**
   #+BEGIN_SRC shell
   # Add timestamp to make unique
   S3_BUCKET="${ACCOUNT_ID}-us-west-2-rag-workshop-$(date +%s)"
   #+END_SRC

4. **OpenSearch Collection Failed**
   - Check IAM permissions
   - Verify network policies allow public access

5. **Knowledge Base Creation Failed**
   - Ensure OpenSearch collection is ACTIVE
   - Check IAM role has correct trust policy

* Summary

This consolidated notebook provides:

1. **Complete Workshop Setup**: All resources needed for Lab 1
2. **Multiple Testing Levels**: From local-only to full AWS integration
3. **Validation Commands**: Easy verification of each component
4. **Cost Tracking**: Built-in cost estimation
5. **Troubleshooting**: Common issues and solutions

Use the weekend testing script to quickly validate your entire setup.

Next module: [[file:02_advanced_rag.org][Advanced RAG Techniques]]